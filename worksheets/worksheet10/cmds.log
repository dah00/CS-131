    8  grep 1576734587 
    9  cd ..
   10  grep 1576734587 PRODUCTS
   11  move PRODUCTS PRODUCTS_old
   12  mv PRODUCTS PRODUCTS_old
   13  ls
   14  mkdir PRODUCTS
   15  mv 1576734587.txt PRODUCTS/
   16  LS
   17  ls
   18  ls PRODUCTS
   19  rm PRODUCTS/1576734587.txt 
   20  cd PRODUCTS
   21  grep 1576734587 ../amazon_reviews_us_Books_v1_02.tsv > 1576734587.txt
   22  ls
   23  cat 1576734587.txt 
   24  awk 'BEGIN {FS = OFS = "\t"} {print $0, "NewColumn"}' 1576734587.txt 
   25  head -3 1576734587.txt 
   26  awk 'BEGIN {FS = OFS = "\t"} {print $0, "5"}' 1576734587.txt 
   27  head -3 1576734587.txt 
   28  head -1 1576734587.txt 
   29  cd ..
   30  worksheets
   31  cd worksheets
   32  cd worksheet6
   33  ls
   34  cd workplace/assignments
   35  cls
   36  ls
   37  mkdir a4
   38  cd a4
   39  cp /home/test/A1/downloaded_tweets_extend_original_nolf2.tsv .
   40  ls
   41  cp /home/test/A1/downloaded_tweets_extend_nolf2.tsv .
   42  ls
   43  ls ../../../datafiles
   44  tmux -l
   45  tmux ls
   46  tmux kill-session -a
   47  tmux ls
   48  tmux kill-session -a
   49  tmux ls
   50  tmux kill-session -t ws6
   51  tmux ls
   52  tmux new -s a4
   53  cd ..
   54  cd worksheets
   55  ls
   56  cd worksheet5
   57  ls
   58  cat ws5.txt 
   59  cd ..
   60  cd worksheet6
   61  ls
   62  cat ws6.txt 
   63  exit
   64  vi test.txt
   65  grep sleep
   66  grep sleep test.txt
   67  grep -F sleep test.txt
   68  head -3 downloaded_tweets_extend_original_nolf2.tsv 
   69  head -3 downloaded_tweets_extend_nolf2.tsv 
   70  cd ..
   71  cd a3
   72  ls
   73  cat a3.txt 
   74  exit
   75  cd workplace/assignments/a4
   76  cut -d "	" -f4 downloaded_tweets_extend_nolf2.tsv | head -10
   77  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | head -10
   78  grep retweeed downloaded_tweets_extend_nolf2.tsv | head 20
   79  grep retweeed downloaded_tweets_extend_nolf2.tsv | head -20
   80  grep retweeted downloaded_tweets_extend_nolf2.tsv | head -20
   81  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | head -20
   82  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | head -20
   83  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" | head -20
   84  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" | sort -n | uniq -c | sort -rn |  head -10
   85  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" | sort -n | uniq -c | sort -rn |  head -10 > top10retweeted.txt
   86  ls
   87  cut -d " " -f2  top10retweeted.txt 
   88  cat top10retweeted.txt 
   89  cut -d ' ' -f2 top10retweeted.txt 
   90  awk -F " " '{print $2}' top10retweeted.txt 
   91  awk -F " " 'BEGIN { ids[i++] = $2; for(i in ids) {print i}}' top10retweeted.txt 
   92  awk -F " " '{ ids[i++] = $2; for(i in ids) {print i}}' top10retweeted.txt 
   93  awk -F " " '{ ids[i++] = $2; for(i in ids) {print ids[i]}}' top10retweeted.txt 
   94  awk -F " " '{ ids[i++] = $2; for(i in ids) {fgrep ids[i]}}' top10retweeted.txt 
   95  awk -F " " '{ ids[i++] = $2; for(i in ids) {fgrep ids[i] downloaded_tweets_extend_original_nolf2.tsv}}' top10retweeted.txt 
   96  awk -F " " '{ ids[i++] = $2; for(i in ids) {`fgrep ids[i] downloaded_tweets_extend_original_nolf2.tsv`}}' top10retweeted.txt 
   97  awk -F " " '{ ids[i++] = $2; for(i in ids) "fgrep ids[i] downloaded_tweets_extend_original_nolf2.tsv"}' top10retweeted.txt 
   98  awk -F " " '{ ids[i++] = $2; for(i in ids) system("fgrep ids[i] downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
   99  awk -F " " '{ ids[i++] = $2; for(i in ids) system("fgrep $ids[i] downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  100  awk -F " " '{ ids[i++] = $2; for(i in ids) system("fgrep $(ids[i]) downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  101  tmux 
  102  awk -F " " '{system("fgrep $2 downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  103  awk -F " " '{system("fgrep "$2" downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  104  head -2 downloaded_tweets_extend_original_nolf2.tsv 
  105  cat top10retweeted.txt 
  106  tmux new -s a4
  107  tmux ls
  108  tmux attach -t a4
  109  exit
  110  cd workplace/assignments/a4
  111  head -3 downloaded_tweets_extend_original_nolf2.tsv 
  112  cut -d "\t" -f5 downloaded_tweets_extend_original_nolf2.tsv 
  113  cut -d '\t' -f5 downloaded_tweets_extend_original_nolf2.tsv 
  114  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv 
  115  grep retweet downloaded_tweets_extend_original_nolf2.tsv 
  116  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv  | grep retweeted
  117  head -3 downloaded_tweets_extend_nolf2.tsv 
  118  grep retweeted downloaded_tweets_extend_nolf2.tsv | head -10
  119  grep retweeted downloaded_tweets_extend_nolf2.tsv | sed -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted//g" | head -20
  120  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted//g" | head -20
  121  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" | head -20
  122  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" > retweetedIDs.txt
  123  kinit
  124  ccrkinit
  125  vi test
  126  ls
  127  rm test.txt
  128  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" > retweetedIDs.txt
  129  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" > retweetedIDs.csv
  130  df =h
  131  df -h
  132  cd ../../..
  133  cd datafiels
  134  cd datafiles
  135  ls
  136  rm downloaded_tweets_extend_nolf2.tsv 
  137  rm downloaded_tweets_extend_original_nolf2.tsv 
  138  cd..
  139  cd  ..
  140  cd workplace/assignments/a4
  141  ls
  142  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5 | sed "s/^.* id=//g" | sed "s/ type=retweeted\]//g" > retweetedIDs.csv
  143  ls
  144  head -5 retweetedIDs.csv 
  145  awk -F "/t" '{print $1}' retweededIDs.csv | head -5
  146  awk -F "/t" '{print $1}' retweetedIDs.csv | head -5
  147  awk -F "/t" '{retweets[i++]=$1} {for(i in retweets) {"fgrep "$1" retweetedIDs.csv | print $1 "," retweets[i]")} | head -5
  148  awk -F "/t" '{retweets[i++]=$1} {for(i in retweets) {"fgrep "$1" downloaded_tweets_extend_nolf2.tsv | print $1 "," retweets[i]")} retweetedIDs.csv | head -5
  149  awk -F "/t" '{retweets[i++]=$1} {for(i in retweets) {"fgrep "$1" downloaded_tweets_extend_nolf2.tsv | print $1 "," retweets[i]")}' retweetedIDs.csv | head -5
  150  rm retweetedIDs.csv 
  151  ls
  152  head -4 downloaded_tweets_extend_original_nolf2.tsv 
  153  grep retweeted downloaded_tweets_extend_original_nolf2.tsv 
  154  grep retweeted downloaded_tweets_extend_nolf2.tsv | head -5
  155  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | head -5 
  156  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/^\[ id=//g" | head -5
  157  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=//g" | head -5
  158  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | head -5
  159  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted//g"  head -5
  160  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted\]//g" | head -5
  161  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted\]//g" > q1.csv
  162  cat q1.csv | cw -l
  163  cat q1.csv | cv -l
  164  cat q1.csv | wc -l
  165  ls
  166  awk -F, 
  167  awk -F, '{if($1!=$2) print $0}' q1.csv | sort -n | uniq -c | sort -k 1 -rn | head -9
  168  awk -F, '{if($1!=$2) print $0}' q1.csv | sort -n | uniq -c | sort -k 1 -rn | awk -F " " '{if($1>=3) {print}}' 
  169  head -20 q1.csv 
  170  cat q1.csv |
  171  cat q1.csv | sort -n | uniq -c | sort -k 1 -rn | awk -F " " '{if($1>=3) {print}}' 
  172  awk -F "," '{if($1!=$2) print $1}' q1.csv | sort -n | uniq -c | sort -k1 -rn | awk -F " " '{if($1>=3) {print}}' 
  173  sed “s/,.*$//g” q1.csv > q2.csv
  174  sed "s/,.*$//g" q1.csv > q2.csv
  175  sort q2.csv | uniq -c | sort -k 1 -n -r
  176  sort q2.csv | uniq -c | sort -k 1 -n
  177  head -5 downloaded_tweets_extend_nolf2.tsv 
  178  grep retweeted downloaded_tweets_extend_nolf2.tsv  | head -5
  179  grep retweeted downloaded_tweets_extend_nolf2.tsv  | wc -l
  180  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,2 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted\]//g" | head -5
  181  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f1,2 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted\]//g" | head -5
  182  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f2,1 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/type=retweeted\]//g" | head -5
  183  exit'
  184  exit
  185  cd workplace/assignments/a3
  186  ls
  187  cat a3.txt 
  188  cd /mnt/scratch
  189  ls
  190  cd obeda
  191  ;s
  192  ls
  193  cd jessie
  194  cd ..
  195  cd jessie
  196  ls
  197  cd ..
  198  cd guoman
  199  ls
  200  cd ..
  201  cd workplace/assignments/a3
  202  ls
  203  cat a3.txt 
  204  cd /
  205  cd home
  206  ls
  207  cd jessie
  208  cd chloe
  209  cd obeeda
  210  cd obeda
  211  man sort
  212  cd workplace/assignments/a3
  213  cat a3.txt 
  214  cd workplace/assignments/a4
  215  ls
  216  head -3 downloaded_tweets_extend_nolf2.tsv 
  217  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,2 | head -10
  218  grep retweeted downloaded_tweets_extend_nolf2.tsv | cut -d "	" -f5,2 | sed "s/$(echo '\t')\[<ReferencedTweet id=/,/g" | sed "s/ type=retweeted\]//g" | head -10
  219  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | head -10
  220  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=retweeted\]//g" | head -10
  221  ls
  222  rm q1.csv 
  223  rm q2.csv 
  224  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=retweeted\]//g" | wc -l
  225  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=retweeted\]//g" > q1.csv
  226  awk -F, '{if($1!=$2) {print $0}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | head -10
  227  tmux ls
  228  awk -F, '{if($1!=$2) {print $0}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | awk -F " " 'if($1>=3) {print}}' 
  229  awk -F, '{if($1!=$2) {print $0}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print}}' 
  230  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' 
  231  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print}}' | head -10
  232  cat q1.csv | wc -l
  233  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -k1 -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' | wc -l
  234  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' | wc -l
  235  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print}}' | head -10
  236  fgrep 
  237  fgrep 1497678663046905863 q1.csv 
  238  fgrep 1516513505696010243 q1.csv 
  239  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' > tweetIDsSup3.txt
  240  cat tweetIDsSup3.txt | wc -l
  241  fgrep -f tweetIDsSup3.txt q1.csv | head -30
  242  fgrep -f tweetIDsSup3.txt q1.csv | sort | head -30
  243  cat tweetIDsSup3.txt | head -10
  244  fgrep -f tweetIDsSup3.txt q1.csv | sort | awk -F, $1!=$2 | head -30
  245  fgrep -f tweetIDsSup3.txt q1.csv | sort | awk -F, '$1!=$2' | head -30
  246  fgrep -f tweetIDsSup3.txt q1.csv | uniq -c | awk -F " " '{print $2}' | head -30
  247  awk 'system("fgrep "$1" q1.csv")' tweetIDsSup3.txt | head -20
  248  awk 'system("fgrep "$1" q1.csv")' tweetIDsSup3.txt | tail -20
  249  awk 'system("fgrep "$1" q1.csv")' tweetIDsSup3.txt > q2.csv
  250  cat q2.csv | wc -l
  251  cut -d "," -f1 q2.csv | uniq -c | head -10
  252  cut -d "," -f1 q2.csv | uniq -c | sort -k1 -rn | head -10
  253  cut -d "," -f1 q2.csv | uniq -c | cut -d " " -f1 | sort -k1 -n | head -10
  254  cut -d "," -f1 q2.csv | uniq -c | cut -d ' ' -f1 | sort -k1 -n | head -10
  255  cut -d "," -f1 q2.csv | uniq -c | cut -d ' ' -f1 | sort -n | head -10
  256  cut -d "," -f1 q2.csv | uniq -c | cut -d " " -f1 | sort -n | head -10
  257  cut -d "," -f1 q2.csv | uniq -c | cut -d " " -f1 | head -10
  258  cut -d "," -f1 q2.csv | uniq -c |  head -10
  259  cut -d "," -f1 q2.csv | uniq -c | cut -f1 |  head -10
  260  cut -d "," -f1 q2.csv | uniq -c | cut -f2 |  head -10
  261  cut -d "," -f1 q2.csv | uniq -c | awk -F " " 'print $1' |  head -10
  262  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' |  head -10
  263  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | sort -n |  head -10
  264  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n  |  head -10
  265  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n  |  tail -10
  266  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n  |  cat
  267  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n > q3.txt
  268  /etc/gnuplot-5.4.4/src/gnuplot 
  269  ls
  270  head -3 downloaded_tweets_extend_original_nolf2.tsv 
  271  head -10 downloaded_tweets_extend_original_nolf2.tsv 
  272  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv | head -10
  273  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv | head -40
  274  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv | tail -40
  275  cut -d "	" -f5 downloaded_tweets_extend_original_nolf2.tsv | tail -80
  276  head -10 downloaded_tweets_extend_original_nolf2.tsv 
  277  head -10 q2.csv 
  278  cut -d "	" -f5 | head -20
  279  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv  | head -20
  280  grep replied_to downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=replied_to//g" | head -5
  281  grep replied_to downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=replied_to\]//g" | head -5
  282  grep replied_to downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=replied_to\]//g" > q5_1
  283  ls
  284  rm q5_1 
  285  grep replied_to downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=replied_to\]//g" > q5_1.csv
  286  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' | head -10
  287  cat q5_1.csv | wc -l
  288  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | sort -rn | head -10
  289  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv |  head -10
  290  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | wc -l
  291  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | wc -l
  292  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | wc -l
  293  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | sort -rn |   head -10
  294  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | tail -10
  295  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' | head -10
  296  fgrep 1447870217615515653 q5_1.csv 
  297  awk -F, '{if($1!=$2) {print $1}}' q5_1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' > q5_2.txt
  298  awk 'sytem("fgrep "$1" q5_1.csv")' q5_2.txt
  299  awk 'system("fgrep "$1" q5_1.csv")' q5_2.txt
  300  awk 'system("fgrep "$1" q5_1.csv")' q5_2.txt > cluster_replies.txt
  301  rm cluster_replies.csv 
  302  awk 'system("fgrep "$1" q5_1.csv")' q5_2.txt > cluster_replies.csv
  303  ls
  304  head -20 q3.txt 
  305  head -20 q2.txt 
  306  head -20 q2.csv 
  307  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | gret retweeted | sed "s/^.* id=//g" | sed "s/type=retweeted\]//g" | sort -n | uniq -c | sort -rn | head -10 > top10retweeted.txt
  308  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | sed "s/type=retweeted\]//g" | sort -n | uniq -c | sort -rn | head -10 > top10retweeted.txt
  309  head -10 top10retweeted.txt 
  310  awk -F " " '{system("fgrep "$2" downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  311  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=retweeted\]//g" > q1.csv
  312  head -5 q1.csv 
  313  cat q1.csv | wc -l
  314  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' > tweetIDsSup3.txt
  315  awk 'system("fgrep "$1" q1.csv")' tweetIDsSup3.txt > q2.csv
  316  head -10 q2.csv 
  317  cat q2.csv | wc -l
  318  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n > q3.txt
  319  /etc/gnuplot-5.4.4/src/gnuplot 
  320  nano a4.txt 
  321  ne a4.txt
  322  vim a4.txt
  323  gnu a4.txt
  324  nano a4.txt 
  325  cat a4.txt 
  326  cat a4.txt | head -100
  327  2R
  328  exit
  329  cd workplace/assignments/a4
  330  ls
  331  rm histogram.svg 
  332  rm q1.csv 
  333  rm q2.csv 
  334  rm q3.txt 
  335  rm q5_1.csv 
  336  rm q5_2.txt 
  337  rm top10retweeted.txt 
  338  rm tweetIDsSup3.txt 
  339  ls
  340  script a4.txt
  341  cd workplace/assignments/a4
  342  ls
  343  cat a4.txt | head -80
  344  cat a4.txt | tail -80
  345  cd workplace/assignments/a4
  346  ls
  347  vi a4.txt 
  348  cat a4.txt 
  349  sed $'s/[^[:print:]\t]//g' a4.txt
  350  cut -d "	" -f5 downloaded_tweets_extend_nolf2.tsv | grep retweeted | sed "s/^.* id=//g" | sed "s/type=retweeted\]//g" | sort -n | uniq -c | sort -rn | head -10 > top10retweeted.txt
  351  head -10 top10retweeted.txt 
  352  awk -F " " '{system("fgrep "$2" downloaded_tweets_extend_original_nolf2.tsv")}' top10retweeted.txt 
  353  grep retweeted downloaded_tweets_extend_nolf2.tsv | awk -F "\t" '{print $5 "," $2}' | sed "s/\[<ReferencedTweet id=//g" | sed "s/ type=retweeted\]//g" > q1.csv
  354  head -5 q1.csv 
  355  cat q1.csv | wc -l
  356  awk -F, '{if($1!=$2) {print $1}}' q1.csv | sort -n | uniq -c | sort -rn | awk -F " " '{if($1>=3) {print $2}}' > tweetIDsSup3.txt
  357  awk 'system("fgrep "$1" q1.csv")' tweetIDsSup3.txt > q2.csv
  358  head -10 q2.csv 
  359  cut -d "," -f1 q2.csv | uniq -c | awk -F " " '{print $1}' | uniq -c | sort -n > q3.txt
  360  /etc/gnuplot-5.4.4/src/gnuplot 
  361  cd workplace/assignments/a4
  362  vi a4.txt
  363  nano a4.txt
  364  col -bp a4.txt | less -R
  365  col -bp typescript | less -R
  366  col -bp < typescript
  367  col -bp < a4
  368  col -bp typescript | less -Rol -bp typescript | less -R
  369  ls
  370  col -bp a4.txt | less -R
  371  cat typescript | perl -pe 's/\e([^\[\]]|\[.*?[a-zA-Z]|\].*?\a)//g'                  | col -b > typescript-processed
  372  cat a4.txt | perl -pe 's/\e([^\[\]]|\[.*?[a-zA-Z]|\].*?\a)//g' \ | col -b > a4.txt
  373  cat a4.txt | perl -pe 's/\e([^\[\]]|\[.*?[a-zA-Z]|\].*?\a)//g' | col -b > a4.txt
  374  cat a4.txt 
  375  a4.txt
  376  cat a4.txt
  377  ls
  378  cat typescript-processed 
  379  cat a4.txt 
  380  rm typescript-processed 
  381  rm a4.txt 
  382  script a4.txt
  383  vi a4.txt 
  384  cp a4.xt a4temp.txt
  385  cp a4.xt ./a4temp.txt
  386  cat a4.txt 
  387  vi a4
  388  vi a4.txt 
  389  cat a4.txt 
  390  head -10 q2.csv 
  391  head -10 q3.csv 
  392  head -10 q3.txt
  393  head -50 q2.csv 
  394  cat a4.txt 
  395  git add a4.txt
  396  git commit -m "submit assignment4"
  397  git push 
  398  git push
  399  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "   " -f14 > review_body.txt
  400  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 > review_body.txt
  401  cat review_body.txt | wc -l
  402  head -5 review_body.txt 
  403  sed -e 's|<..*/>||g' -e 's/[,.;]//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' review_body.txt > output.txt
  404  head -1- output.txt 
  405  head -10 output.txt 
  406  ls
  407  head -3 amazon_reviews_us_Books_v1_02.tsv 
  408  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | wc -l
  409  grep 1576734587 amazon_reviews_us_Books_v1_02.tsv | head -5
  410  grep 1576734587 amazon_reviews_us_Books_v1_02.tsv | wc -l
  411  head -1 amazon_reviews_us_Books_v1_02.tsv 
  412  cut -d "	" -f14 amazon_reviews_us_Books_v1_02.tsv | head -4
  413  head -2 amazon_reviews_us_Books_v1_02.tsv 
  414  grep 0385730586 amazon_reviews_us_Books_v1_02.tsv | wc -k
  415  grep 0385730586 amazon_reviews_us_Books_v1_02.tsv | wc -l
  416  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | wc -l
  417  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 | head -2
  418  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 | head -10
  419  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 | sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' > output.txt
  420  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 > review_body.txt
  421  cat review_body | wc -l
  422  review_body
  423  cat review_body
  424  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 | head -4
  425  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 > review_body.txt
  426  ls
  427  cat review_body.txt | wc -l
  428  head -3 review_body.txt 
  429  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' review_body.txt > output.txt
  430  ls
  431  cat review_body | sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' > output.txt
  432  cat review_body.txt | sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' > output.txt
  433  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' review_body.txt > output.txt
  434  review_body.txt
  435  grep 0811828964 amazon_reviews_us_Books_v1_02.tsv | cut -d "	" -f14 > review_body
  436  ls
  437  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' review_body > output.txt
  438  cat review_body | wc -l
  439  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' review_body > output.txt
  440  sed 's|<..*/>||g' | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g' | sed 's/if//g' | sed 's/in//g' | sed 's/?//g' review_body.txt > output.txt
  441  sed 's|<..*/>||g' | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g'  review_body.txt > output.txt
  442  sed 's|<..*/>||g' review_body.txt | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g' | sed 's/if//g' | sed 's/in//g' | sed 's/?//g' > output.txt
  443  sed 's|<..*/>||g' review_body.txt | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g' | sed 's/if//g' | sed 's/in//g' | sed 's/?//g' > output.txt
  444  sed 's|<..*/>||g' review_body.txt | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g' | sed 's/if//g' | sed 's/in//g' | sed 's/?//g' > output
  445  ls
  446  sed 's|<..*/>||g' review_body.txt | sed 's/[,.;']//g' | sed 's/it//g' | sed 's/and//g' | sed 's/or//g' | sed 's/if//g' | sed 's/in//g' | sed 's/?//g' review_body > output
  447  sed -e 's|<..*/>||g' -e 's/[,.;]//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' review_body > output.txt
  448  head -10 output.txt 
  449  ls
  450  rm output.txt
  451  rm review_body
  452  rm review_body.txt 
  453  script ws7.txt
  454  history > cmds.log
  455  ls
  456  cat ws7.txt 
  457  git add ws7.txt cmds.log 
  458  git commit -m "Submit worksheet7"
  459  git push 
  460  git push
  461  git pull
  462  git push 
  463  git pull
  464  git stash 
  465  git clean -d -f .
  466  ls
  467  git status
  468  git push 
  469  git fetch 
  470  git branch
  471  git checkout -f main
  472  git checkout -f origin/main
  473  git branch
  474  git checkout main
  475  git fetch 
  476  ls
  477  cd ..
  478  ls
  479  git checkout main
  480  ls
  481  cd worksheet7
  482  ls
  483  git statuts
  484  git status
  485  git pull main
  486  ls
  487  git pull origin/main
  488  git pull
  489  git status
  490  git push 
  491  git status
  492  cd ../../../
  493  cd datafiles/
  494  ls
  495  cp amazon_reviews_us_Books_v1_02.tsv ../workplace/worksheets/worksheet7
  496  cd ../workplace/worksheets/worksheet7
  497  ls
  498  cd ..
  499  cd worksheet6
  500  ls
  501  cd PRODUCTS
  502  ls
  503  cd ..
  504  cd temp
  505  cd ~
  506  vi test
  507  sed 's/[<>,.;]//g' test > result
  508  ls
  509  cat result
  510  cat test
  511  rm result
  512  sed 's/[<>,.;]//g;s/<.../>//g' test > result
  513  sed 's/[<>,.;]//g;s/<...\/>//g' test > result
  514  cat result
  515  rm result
  516  sed 's/[<>,.;]//g;s|<.../>||g' test > result
  517  cat result
  518  cat test
  519  rm result
  520  sed 's|[<>,.;]||g;s|<.../>||g' test > result
  521  rm result
  522  sed 's|[<>,.;]||g;s|<.../>||g' test > result
  523  cat result
  524  vi test
  525  cat test
  526  sed 's|<.../>||g' test > result
  527  cat result
  528  sed 's|<.../>||g' test | sed -e 's/it//g' -e 's/and//g' > result
  529  cat result
  530  rm result
  531  sed 's|<.../>||g' test | sed -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' > result
  532  cat result
  533  rm result
  534  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' > result
  535  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/[,.;]/'  test > result
  536  cat result
  537  rm result
  538  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/[,.;]/'  test > result
  539  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/[,.;]//g'  test > result
  540  cat test
  541  cat result
  542  ls
  543  rm result
  544  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > output.txt
  545  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' test > output.txt
  546  sed -e 's|<.../>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' test > output.txt
  547  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/[,.;]//g'  test > result
  548  rm result
  549  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > output.txt
  550  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > output
  551  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > result
  552  ls
  553  sed -e 's|<..*/>||g' -e 's/[,.;']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g'  test > result
  554  sed -e 's|<.../>||g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/[,.;]//g'  test > result
  555  ls 
  556  rm result
  557  sed -e 's|<..*/>||g' -e 's/[,.;\']//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > output
  558  sed -e 's|<..*/>||g' -e 's/[,.;]//g' -e 's/it//g' -e 's/and//g' -e 's/or//g' -e 's/if//g' -e 's/in//g' -e 's/?//g' test > output
  559  rm output
  560  cd workplace/worksheets
  561  ls
  562  mkdir worksheet7
  563  cd worksheet7
  564  tmux -s ws7
  565  tmux new -s ws7
  566  ls
  567  cd datafiles
  568  ls
  569  cd workplace/worksheets
  570  ls
  571  mkdir worksheet8
  572  cd /mnt/scratch
  573  ls
  574  cd obeda
  575  ls
  576  git status
  577  mkdir worksheets
  578  cd worksheets
  579  mkdir worksheets8
  580  ls
  581  rm worksheets8/
  582  rm worksheets8
  583  rm -d worksheets8
  584  ls
  585  mkdri worksheet8
  586  mkdir worksheet8
  587  cd worksheet8
  588  cp ~/datafiles/amazon_reviews_us_Books_v1_02.tsv .
  589  head -2 amazon_reviews_us_Books_v1_02.tsv 
  590  cut -d "	" -f12 | head -10
  591  cut -d "	" -f12 amazon_reviews_us_Books_v1_02.tsv | head -10
  592  awk -F "\t" '$12="Y"' amazon_reviews_us_Books_v1_02.tsv | head -20
  593  awk -F "\t" '"' amazon_reviews_us_Books_v1_02.tsv | head -20
  594  awk -F "\t" '$12="Y"' amazon_reviews_us_Books_v1_02.tsv > verified.txt
  595  cat verified.txt | wc -l
  596  awk -F "\t" '$12="Y"' amazon_reviews_us_Books_v1_02.tsv > verified.txt
  597  awk -F "\t" '$12="N"' amazon_reviews_us_Books_v1_02.tsv > unverified.txt
  598  cat verified.txt | wc -l
  599  cat unverified.txt | wc -l
  600  cut -d "       " -f14 verified.txt | head -100 |  sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formattedverified.txt
  601  cat verified.txt | head -100 |  sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formattedverified.txt
  602  ls
  603  cat unverified.txt | head -100 |  sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formattedunverified.txt
  604  ls
  605  cat formattedverified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -40
  606  cat formattedverified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -60
  607  cat formattedunverified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -60
  608  cat formattedunverified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -70
  609  cd /etc/scratch
  610  cd /mnt/scratch
  611  ls
  612  cd obeda
  613  ls
  614  cd worksheets
  615  ls
  616  cd worksheet8
  617  ls
  618  awk -F "\t" '$12="N"' amazon_reviews_us_Books_v1_02.tsv > unverified.txt
  619  ls
  620  cat unverified.txt | wc -l
  621  head -3 verified.txt 
  622  head -3 amazon_reviews_us_Books_v1_02.tsv 
  623  cut -d "	" -f14 amazon_reviews_us_Books_v1_02.tsv | head -4
  624  ls
  625  tr -c '[:alnum]' '[\n*]' < head -100 verified.txt | sort | uniq -c | sort -nr | head -20
  626   head -100 verified.txt | tr -c '[:alnum]' '[\n*]' | sort | uniq -c | sort -nr | head -20
  627   head -100 verified.txt | tr -c '[:alnum]' '[\n*]' | sort | uniq -c | sort -nr | head -40
  628  head -2 verified.txt 
  629   head -100 verified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -40
  630   sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' verified.txt 
  631  cut -d "	" -f14 amazon_reviews_us_Books_v1_02.tsv | sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formatedverified.txt
  632  cat formatedverified.txt | wc -l
  633  cut -d "	" -f14 verified.txt | head -100 |  sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formatedverified.txt
  634  ls
  635  rm formatedverified.txt 
  636  cut -d "	" -f14 verified.txt | head -100 |  sed -e 's/the//g' -e 's/of//g' -e 's/and//g' -e 's/to//g' -e 's/a//g' -e 's/is//g' -e 's/in//g' -e 's/I//g' -e 's/br//g' -e 's/that//g' -e 's/it//g' -e 's/this//g' -e 's/for//g' > formatedverified.txt
  637  ls
  638  cat formatedverified.txt | wc -l
  639  cat formatedverified.txt | tr -c '[:alnum:]' '[\n*]' | sort | uniq -c | sort -nr | head -40
  640  ls
  641  rm unverified.txt 
  642  rm verified.txt 
  643  rm formatedverified.txt 
  644  script ws8
  645  vi formattedverified.txt 
  646  vi ws8 
  647  script -a ws8
  648  vi ws8
  649  cat ws8 
  650  ls
  651  head -10 ws8 
  652  history > cmds.log
  653  ls
  654  cd ..
  655  ls
  656  cd ..
  657  ls
  658  git init
  659  git status
  660  git remote add origin https://github.com/dah00/CS-131---Processing-Big-Data.git
  661  git branch
  662  git branches
  663  git branch
  664  git add worksheets/worksheet8/ws8 worksheets/worksheet8/cmds.log 
  665  git status
  666  git commit -m "submit worksheet8"
  667  git push 
  668  git push origin main 
  669  git push --set-upstream origin master
  670  i1=1
  671  i2=2
  672  test $i1 -eq $i2
  673  echo $?
  674  i2=1
  675  echo $?
  676  test $i1 -eq $i2
  677  echo $?
  678  a="1111"
  679  echo $a
  680  test -z $a
  681  echo $?
  682  test -z $b
  683  echo $?
  684  type cd
  685  type mv
  686  type expr
  687  type mkdir
  688  vi minimum.sh
  689  cd /mnt/scratch
  690  cd obeda
  691  ls
  692  vi minimum.sh
  693  ls
  694  rm minimum.sh 
  695  df -h
  696  whoami
  697  cd ..
  698  df -h
  699  man echo
  700  who
  701  date
  702  date | grep Wed
  703  i=5
  704  $5
  705  echo "$i"
  706  echo "$i times 5 is $(( $i * 5))"
  707  result=`expr $i +4`
  708  result=`expr $i+4`
  709  echo "$result"
  710  result=`expr ($i+4)`
  711  result=`expr $i + 4`
  712  echo "$result"
  713  man echo
  714  read x y z < echo "What a wonderful day!"
  715  read x y z 
  716  echo z y x
  717  echo "z y x"
  718  echo "$z $y $x"
  719  type echo
  720  type cd
  721  type expr
  722  type read
  723  type rmdir
  724  ls
  725  cd /mnt/scratch/
  726  ls
  727  cd obeda
  728  vi test.txt
  729  awk 'ssn { pring }' test.txt 
  730  awk '/ssn { pring }' test.txt 
  731  awk '/ssn/ { pring }' test.txt 
  732  awk 'ssn { print }' test.txt 
  733  awk '/ssn { print }' test.txt 
  734  awk '/ssn/ { print }' test.txt 
  735  sed '3q' test.txt 
  736  head -3 test.txt 
  737  du
  738  cat < test.txt 
  739  man cp
  740  echo "obeda"
  741  echo -n "Obeda "
  742  man echo
  743  man xargs
  744  rand=$(($RANDOM % 100))
  745  echo "$rand"
  746  vi randomsample.sh
  747  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  748  chmod 774
  749  chmod 774 randomsample.sh 
  750  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  751  rand=$(($RANDOM % 100))
  752  echo "$rand"
  753  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  754  cd /mnt/scratch
  755  ;s
  756  ls
  757  cd obeda
  758  ls
  759  vi test.txt
  760  rm test.txt 
  761  ls
  762  cd worksheets
  763  ls
  764  mkdir worksheet9
  765  cd worksheet8
  766  ls
  767  cp amazon_reviews_us_Books_v1_02.tsv ../worksheet9/
  768  cd ../worksheet9
  769  ls
  770  vi randomsample.sh
  771  ./randomsample.sh 5 dfs.txt
  772  chmor 774 randomsample.sh 
  773  chmod 774 randomsample.sh 
  774  ./randomsample.sh 5 dfs.txt
  775  vi randomsample.sh 
  776  ./randomsample.sh 5 dfs.txt
  777  vi randomsample.sh 
  778  ./randomsample.sh 5 dfs.txt
  779  vi randomsample.sh 
  780  ./randomsample.sh 5 dfs.txt
  781  vi randomsample.sh 
  782  ./randomsample.sh 5 dfs.txt
  783  vi randomsample.sh 
  784  ./randomsample.sh 5 dfs.txt
  785  vi randomsample.sh 
  786  echo $(($RANDOM % 100))
  787  vi randomsample.sh 
  788  ./randomsample.sh echo $(($RANDOM % 100)) amazon_reviews_us_Books_v1_02.tsv 
  789  vi randomsample.sh 
  790  ./randomsample.sh echo $(($RANDOM % 100)) amazon_reviews_us_Books_v1_02.tsv 
  791  vi randomsample.sh 
  792  ./randomsample.sh echo $(($RANDOM % 100)) amazon_reviews_us_Books_v1_02.tsv 
  793  vi randomsample.sh 
  794  ./randomsample.sh echo $(($RANDOM % 100)) amazon_reviews_us_Books_v1_02.tsv 
  795  rand=echo $(($RANDOM % 100))
  796  rand=`echo $(($RANDOM % 100))`
  797  echo "$rand"
  798  ./randomsample.sh rand amazon_reviews_us_Books_v1_02.tsv 
  799  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  800  vi randomsample.sh 
  801  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  802  vi randomsample.sh 
  803  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  804  wq
  805  vi randomsample.sh 
  806  wq
  807  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  808  vi randomsample.sh 
  809  ./randomsample.sh $rand amazon_reviews_us_Books_v1_02.tsv 
  810  vi randomsample.sh 
  811  ls
  812  rm randomsample.sh 
  813  script ws9.txt
  814  history > cmds.log
  815  git status
  816  ls
  817  git add randomsample.sh ws9.txt
  818  git commit -m "submitting worksheet9"
  819  git push 
  820  ls
  821  vi ws9.txt 
  822  cat ws9.txt 
  823  git tag -l
  824  git reset --hard submitting worksheet9
  825  git reset --hard 969abbc4fa1c3a9bb4622009918b8915c5fabb48
  826  git reset HEAD~
  827  git status
  828  git log
  829  ls
  830  git add ws9.txt cmds.log 
  831  git commit -m "Submit ws9"
  832  git push 
  833  git pull
  834  git push 
  835  git pull
  836  git pull master
  837  git pull -branch master
  838  git pull
  839  git status
  840  git branch
  841  git stash
  842  git clean -f
  843  ls
  844  git status
  845  git push 
  846  git pull
  847  git push
  848  exit
  849  cd workplace
  850  cd assignments
  851  cd ls
  852  ls
  853  cd a4
  854  ls
  855  cat tweetIDsSup3.txt | wc -l
  856  head tweetIDsSup3.txt 
  857  head cluster_replies.csv 
  858  cd workplace/assignments/a3
  859  ls
  860  cd ../a3
  861  cd ../a4
  862  ls
  863  head -2 downloaded_tweets_extend_original_nolf2.tsv 
  864  ls
  865  cp downloaded_tweets_extend_nolf2.tsv /mnt/scratch/obeda
  866  cp downloaded_tweets_extend_original_nolf2.tsv /mnt/scratch/obeda
  867  cd workplace/assignments/a3
  868  ls
  869  cat Q2.csv | wc -l
  870  cat Q2.csv 
  871  mv Q2.csv cluster_replies.csv
  872  ls
  873  cp cluster_replies.csv /mnt/scratch/obeda/
  874  cd ../a4
  875  ls
  876  cat cluster_replies.csv | wc -l
  877  cat cluster_replies.csv
  878  cat q2.csv | wc -l
  879  head q2.csv 
  880  head tweetIDsSup3.txt 
  881  ls
  882  head q1.csv 
  883  head q3.csv 
  884  head q3.txt 
  885  cd ../a3
  886  ls
  887  cat a3.txt 
  888  ls
  889  cd mnt/scratch 
  890  cd /mnt/scratch
  891  cd obeda
  892  ls
  893  mkdir assignments
  894  ls
  895  mkdir assignmnets/a5
  896  cd assignments
  897  mkdir a5
  898  ls
  899  cd ..
  900  mv downloaded_tweets_extend_nolf2.tsv assignments/a5
  901  mv downloaded_tweets_extend_original_nolf2.tsv assignments/a5
  902  ls
  903  cd assignments/a5
  904  ls
  905  grep type=replied_to downloaded_tweets_extend_original_nolf2.tsv | awk -F '\t' '{print $6,$2}' | sort > a3_q1.txt
  906  cat a3_q1.txt | wc -l
  907  head a3_q1.txt 
  908  grep type=replied_to downloaded_tweets_extend_original_nolf2.tsv | awk -F '\t' '{print $6,$4}' | sort | uniq -c | sort -r | awk '{ if ($1 >= 3) {print} }' > a3_q2.tsv
  909  cat a3_q2.tsv | wc -l
  910  cat a3_q2.tsv 
  911  head -2 downloaded_tweets_extend_original_nolf2.tsv 
  912  cd /mnt/scratch
  913  ls
  914  cd obeda
  915  ls
  916  cd worksheets
  917  ls
  918  mkdir worksheet10
  919  ls
  920  sftp
  921  sftp obeda@172.31.197.164
  922  ls
  923  cd worksheet9
  924  ls
  925  cd ..
  926  cd worksheet8
  927  ls
  928  cp amazon_reviews_us_Books_v1_02.tsv ../workhseet10/
  929  cp amazon_reviews_us_Books_v1_02.tsv ../workhseet10
  930  cp amazon_reviews_us_Books_v1_02.tsv ../worksheet10
  931  cd ..
  932  cd worksheet10
  933  ls
  934  head amazon_reviews_us_Books_v1_02.tsv 
  935  ls
  936  cat numbers.py 
  937  time python3 numbers.py
  938  vi numbers.sh
  939  vi numbers.
  940  vi numbers.sh
  941  head -2 amazon_reviews_us_Books_v1_02.tsv 
  942  vi numbers.sh
  943  chmod 774 numbers.sh
  944  ./numbers.sh
  945  vi numbers.sh
  946  ./numbers.sh
  947  vi numbers.sh
  948  ./numbers.sh
  949  vi numbers.sh
  950  ./numbers.sh
  951  vi numbers.sh
  952  awk '{ print $9}' | head -10
  953  awk '{ print $9}' amazon_reviews_us_Books_v1_02.tsv  | head -10
  954  head -1 amazon_reviews_us_Books_v1_02.tsv 
  955  awk '{ print $9}' amazon_reviews_us_Books_v1_02.tsv  | head -10
  956  awk -F "\t" '{ print $9}' amazon_reviews_us_Books_v1_02.tsv  | head -10
  957  vi numbers.sh
  958  ./numbers.sh 
  959  vi numbers.sh
  960  ./numbers.sh 
  961  vi numbers.sh
  962  ./numbers.sh 
  963  vi numbers.sh
  964  ./numbers.sh 
  965  vi numbers.sh
  966  ./numbers.sh 
  967  vi numbers.sh
  968  ./numbers.sh 
  969  vi numbers.sh
  970  ./numbers.sh 
  971  vi numbers.sh
  972  ./numbers.sh 
  973  vi numbers.sh
  974  ./numbers.sh 
  975  vi numbers.sh
  976  ./numbers.sh 
  977  vi numbers.sh
  978  time ./numbers.sh 
  979  time python3 numbers.py
  980  vi numbers.sh
  981  time python3 numbers.py
  982  time ./numbers.sh 
  983  vi numbers.sh
  984  time ./numbers.sh 
  985  time python3 numbers.py
  986  cd mnt/scratch
  987  cd mn
  988  cd mnt
  989  ls
  990  cd mnt
  991  cd /
  992  ls
  993  cd mnt
  994  cd scratch
  995  cd obeda
  996  cd worksheet10
  997  ls
  998  cd workhseets
  999  cd worksheets
 1000  cd worksheet10
 1001  ls
 1002  cd ../../
 1003  script ws10.txt
 1004  vi ws10.txt
 1005  tail ws10.txt
 1006  vi ws10.txt
 1007  history > cmds.log
